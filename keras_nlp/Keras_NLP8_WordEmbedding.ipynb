{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings in Keras\n",
    "\n",
    "A word embedding is a class of approaches for representing words and documents using a\n",
    "dense vector representation. It is an improvement over more the traditional bag-of-word model\n",
    "encoding schemes where large sparse vectors were used to represent each word or to score each\n",
    "word within a vector to represent an entire vocabulary. These representations were sparse\n",
    "because the vocabularies were vast and a given word or document would be represented by a\n",
    "large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, in an embedding, words are represented by dense vectors where a vector represents\n",
    "the projection of the word into a continuous vector space. The position of a word within the\n",
    "vector space is learned from text and is based on the words that surround the word when it is\n",
    "used. The position of a word in the learned vector space is referred to as its embedding. Two\n",
    "popular examples of methods of learning word embeddings from text include:\n",
    "- Word2Vec.\n",
    "- GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding Layer\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data. It requires\n",
    "that the input data be integer encoded, so that each word is represented by a unique integer.\n",
    "This data preparation step can be performed using the Tokenizer API also provided with\n",
    "Keras.\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all\n",
    "of the words in the training dataset.\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3\n",
    "arguments:\n",
    "- input dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words\n",
    "- output dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "- input length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning an Embedding\n",
    "Learn a word embedding while fitting a neural\n",
    "network on a text classification problem. Define a small problem where we have 10\n",
    "text documents, each with a comment about a piece of work a student submitted. Each text\n",
    "document is classified as positive 1 or negative 0. This is a simple sentiment analysis problem.\n",
    "First, we will define the documents and their class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richard\\Anaconda3\\envs\\tf15\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!',\n",
    "'Weak',\n",
    "'Poor effort!',\n",
    "'not good',\n",
    "'poor work',\n",
    "'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 39], [18, 36], [27, 12], [24, 36], [32], [17], [10, 12], [38, 18], [10, 36], [31, 17, 39, 4]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents, could use more complex like TF-IDF, needs to be integer for embedding layer\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 39  0  0]\n",
      " [18 36  0  0]\n",
      " [27 12  0  0]\n",
      " [24 36  0  0]\n",
      " [32  0  0  0]\n",
      " [17  0  0  0]\n",
      " [10 12  0  0]\n",
      " [38 18  0  0]\n",
      " [10 36  0  0]\n",
      " [31 17 39  4]]\n"
     ]
    }
   ],
   "source": [
    "#Keras prefers same lengths so pad \n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs,maxlen=max_length,padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Embedding layer.\n",
    "The Embedding layer has a vocabulary of 50 and an input length of 4. Choose a\n",
    "small embedding space of 8 dimensions. The model is a simple binary classification model.\n",
    "Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one\n",
    "for each word. Flatten this to a one 32-element vector to pass on to the Dense output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,8,input_length = max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#compile\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "#summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the Embedding layer is a 4 x 8 matrix and this is squashed to a 32-element vector\n",
    "by the Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(padded_docs,labels,epochs=50,verbose=0)\n",
    "#evaluate\n",
    "loss,accuracy = model.evaluate(padded_docs,labels,verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simple model not surprising accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
